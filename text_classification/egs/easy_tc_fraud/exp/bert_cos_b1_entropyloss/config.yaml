batch_size: 256
check_data: false
checkpointer:
  args:
    checkpoints_dir: <exp>/models
    recoverables:
      classifier: <classifier>
      embedding_model: <embedding_model>
      epoch_counter: <epoch_counter>
  obj: dguard_nlp.utils.checkpoint.Checkpointer
classifier:
  args:
    input_dim: <embed_dim>
    inter_dim: 512
    num_blocks: 1
    out_neurons: <num_class>
  obj: dguard_nlp.models.classify.CosineClassifier
device: cuda:0
embed_dim: 768
embedding_model:
  args:
    device: <device>
  obj: dguard_nlp.models.bert.BertClassificationModel
epoch_counter:
  args:
    limit: <epochs>
  obj: dguard_nlp.utils.epoch.EpochCounter
epochs: 200
exp: bert_softlabel
log_batch_freq: 600
loss:
  args:
    name: EntropyLoss
  obj: dguard_nlp.loss.margin_loss.EntropyLoss
lr: 0.0005
lr_scheduler:
  args:
    fix_epoch: <epochs>
    max_lr: <lr>
    min_lr: <min_lr>
    optimizer: <optimizer>
    step_per_epoch: null
    warmup_epoch: 5
  obj: dguard_nlp.process.scheduler.WarmupCosineScheduler
margin_scheduler:
  args:
    criterion: <loss>
    final_margin: 0.2
    fix_epoch: 25
    increase_start_epoch: 15
    initial_margin: 0.0
    step_per_epoch: null
  obj: dguard_nlp.process.scheduler.MarginScheduler
min_lr: 5.0e-05
num_class: 2
optimizer:
  args:
    lr: <lr>
    params: null
  obj: transformers.AdamW
seed: 123
test_csv: null
train_csv: ../../data/output.csv
valid_interval: 1
