exp: bert_cbloss
epochs: 205
seed: 123
device: cuda:7

# DATA
train_csv: ../../data/train.csv
test_csv: ../../data/test.csv
batch_size: 300
check_data: false

# FREQ
valid_interval: 1
log_batch_freq: 600

# LR
min_lr: 0.00005
lr: 0.0005

# BERT
embed_dim: 768
num_class: 2

optimizer:
  obj: transformers.AdamW
  args:
    params:
    lr: <lr>

margin_scheduler:
  obj: dguard_nlp.process.scheduler.MarginScheduler
  args:
    criterion: <loss>
    initial_margin: 0.0
    final_margin: 0.2
    increase_start_epoch: 15
    fix_epoch: 25
    step_per_epoch:

epoch_counter:
  obj: dguard_nlp.utils.epoch.EpochCounter
  args:
    limit: <epochs>

checkpointer:
  obj: dguard_nlp.utils.checkpoint.Checkpointer
  args:
    checkpoints_dir: <exp>/models
    recoverables:
      embedding_model: <embedding_model>
      classifier: <classifier>
      epoch_counter: <epoch_counter>

embedding_model:
  obj: dguard_nlp.models.bert.BertClassificationModel
  args:
    device: <device>

classifier:
  obj: dguard_nlp.models.classify.CosineClassifier
  args:
    input_dim: <embed_dim>
    num_blocks: 1
    inter_dim: 512
    out_neurons: <num_class>

# classifier:
#   obj: dguard_nlp.models.classify.LinearClassifier
#   args:
#     input_dim: <embed_dim>
#     num_blocks: 0
#     inter_dim: 512
#     out_neurons: <num_class>

# classifier:
#   obj: dguard_nlp.models.classify.
#   args:
#     input_dim: <embed_dim>
#     num_blocks: 0
#     inter_dim: 512
#     out_neurons: <num_class>

lr_scheduler:
  obj: dguard_nlp.process.scheduler.WarmupCosineScheduler
  args:
    optimizer: <optimizer>
    min_lr: <min_lr>
    max_lr: <lr>
    warmup_epoch: 5
    fix_epoch: <epochs>
    step_per_epoch:

loss:
  obj: dguard_nlp.loss.margin_loss.CrossEntropyLoss
  args:
    name: haha
